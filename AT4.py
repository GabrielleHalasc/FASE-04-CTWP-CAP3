#4. Interpretar os resultados e extrair insights relevantes: 

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Resultados obtidos
resultados = {
    "KNN": {
        "Melhores HiperparÃ¢metros": {'n_neighbors': 1, 'weights': 'uniform'},
        "MÃ©tricas": {
            "AcurÃ¡cia": 0.89,
            "PrecisÃ£o": 0.89,
            "Recall": 0.89,
            "F1-Score": 0.89,
            "Matriz de ConfusÃ£o": [[17, 0, 3], [2, 19, 0], [2, 0, 20]],
        },
    },
    "SVM": {
        "Melhores HiperparÃ¢metros": {'C': 1, 'kernel': 'linear'},
        "MÃ©tricas": {
            "AcurÃ¡cia": 0.90,
            "PrecisÃ£o": 0.91,
            "Recall": 0.90,
            "F1-Score": 0.91,
            "Matriz de ConfusÃ£o": [[17, 0, 3], [1, 20, 0], [2, 0, 20]],
        },
    },
    "Random Forest": {
        "Melhores HiperparÃ¢metros": {'max_depth': 20, 'n_estimators': 5},
        "MÃ©tricas": {
            "AcurÃ¡cia": 0.87,
            "PrecisÃ£o": 0.88,
            "Recall": 0.87,
            "F1-Score": 0.87,
            "Matriz de ConfusÃ£o": [[17, 0, 3], [1, 20, 0], [4, 0, 18]],
        },
    },
    "RegressÃ£o LogÃ­stica": {
        "Melhores HiperparÃ¢metros": {'C': 10, 'solver': 'lbfgs'},
        "MÃ©tricas": {
            "AcurÃ¡cia": 0.92,
            "PrecisÃ£o": 0.92,
            "Recall": 0.92,
            "F1-Score": 0.92,
            "Matriz de ConfusÃ£o": [[17, 0, 3], [0, 21, 0], [2, 0, 20]],
        },
    },
}

# Insights
def extrair_insights(resultados):
    print("AnÃ¡lise dos Resultados")
    print("=" * 40)


    modelos = []
    acuracias = []
    precisaoes = []
    recalls = []
    f1_scores = []

    for modelo, detalhes in resultados.items():
        print(f"Modelo: {modelo}")
        print(f"Melhores HiperparÃ¢metros: {detalhes['Melhores HiperparÃ¢metros']}")

        # MÃ©tricas
        acuracia = detalhes['MÃ©tricas']['AcurÃ¡cia']
        precisao = detalhes['MÃ©tricas']['PrecisÃ£o']
        recall = detalhes['MÃ©tricas']['Recall']
        f1_score = detalhes['MÃ©tricas']['F1-Score']
        matriz_confusao = detalhes['MÃ©tricas']['Matriz de ConfusÃ£o']

        
        modelos.append(modelo)
        acuracias.append(acuracia)
        precisaoes.append(precisao)
        recalls.append(recall)
        f1_scores.append(f1_score)

        # InterpretaÃ§Ã£o
        print(f"AcurÃ¡cia: {acuracia:.2f} - Este modelo classifica corretamente {acuracia * 100:.2f}% das amostras.")
        print(f"PrecisÃ£o: {precisao:.2f} - As classificaÃ§Ãµes positivas sÃ£o corretas em {precisao * 100:.2f}% dos casos.")
        print(f"Recall: {recall:.2f} - O modelo identifica corretamente {recall * 100:.2f}% das amostras de cada classe.")
        print(f"F1-Score: {f1_score:.2f} - EquilÃ­brio entre precisÃ£o e recall.")

        # Matriz de ConfusÃ£o
        print("Matriz de ConfusÃ£o:")
        for linha in matriz_confusao:
            print(linha)

        
        if acuracia >= 0.90:
            print("ğŸŸ¢ Este modelo Ã© altamente eficaz para o problema de classificaÃ§Ã£o de grÃ£os.")
        elif 0.85 <= acuracia < 0.90:
            print("ğŸŸ¡ Este modelo Ã© aceitÃ¡vel, mas pode nÃ£o ser ideal para a automaÃ§Ã£o total.")
        else:
            print("ğŸ”´ Este modelo apresenta desempenho insuficiente para uso prÃ¡tico.")

        print("-" * 40)

    # GrÃ¡ficos de barras para as mÃ©tricas
    x = range(len(modelos))

    plt.figure(figsize=(12, 8))

    plt.subplot(2, 2, 1)
    plt.bar(x, acuracias, color='blue')
    plt.xticks(x, modelos)
    plt.ylabel('AcurÃ¡cia')
    plt.title('AcurÃ¡cia dos Modelos')

    plt.subplot(2, 2, 2)
    plt.bar(x, precisaoes, color='green')
    plt.xticks(x, modelos)
    plt.ylabel('PrecisÃ£o')
    plt.title('PrecisÃ£o dos Modelos')

    plt.subplot(2, 2, 3)
    plt.bar(x, recalls, color='red')
    plt.xticks(x, modelos)
    plt.ylabel('Recall')
    plt.title('Recall dos Modelos')

    plt.subplot(2, 2, 4)
    plt.bar(x, f1_scores, color='purple')
    plt.xticks(x, modelos)
    plt.ylabel('F1-Score')
    plt.title('F1-Score dos Modelos')

    plt.tight_layout()
    plt.show()


extrair_insights(resultados)